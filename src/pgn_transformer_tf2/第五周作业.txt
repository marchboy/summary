
1 补全 layers.transformer.scaled_dot_product_attention 计算 self-attention 函数
2 补全 layers.transformer.MultiHeadAttention.call 函数的代码
3 补全 encoders.self_attention_encoder.EncoderLayer.call 函数，完成 encoder 计算流程
4 使用 bin.main.py 运行模型，调整超参
