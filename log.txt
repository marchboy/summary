nohup: ignoring input
Building prefix dict from the default dictionary ...
2021-03-31 17:17:39,860 : DEBUG : Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
2021-03-31 17:17:39,860 : DEBUG : Loading model from cache /tmp/jieba.cache
Loading model cost 0.782 seconds.
2021-03-31 17:17:40,642 : DEBUG : Loading model cost 0.782 seconds.
Prefix dict has been built successfully.
2021-03-31 17:17:40,642 : DEBUG : Prefix dict has been built successfully.
2021-03-31 17:17:40.717782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-03-31 17:17:40.795329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: TITAN Xp computeCapability: 6.1
coreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s
2021-03-31 17:17:40.795730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-03-31 17:17:40.798444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-03-31 17:17:40.800739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-03-31 17:17:40.801221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-03-31 17:17:40.804032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-03-31 17:17:40.805566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-03-31 17:17:40.810915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-03-31 17:17:40.827454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2021-03-31 17:17:40.828097: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-03-31 17:17:40.842436: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2099940000 Hz
2021-03-31 17:17:40.843807: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f3df0000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-03-31 17:17:40.843833: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-03-31 17:17:41.338380: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b7411e9400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-03-31 17:17:41.338428: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2021-03-31 17:17:41.339999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: TITAN Xp computeCapability: 6.1
coreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s
2021-03-31 17:17:41.340060: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-03-31 17:17:41.340086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-03-31 17:17:41.340109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-03-31 17:17:41.340131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-03-31 17:17:41.340153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-03-31 17:17:41.340174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-03-31 17:17:41.340214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-03-31 17:17:41.369741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2021-03-31 17:17:41.369795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-03-31 17:17:41.372221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-31 17:17:41.372243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2021-03-31 17:17:41.372253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2021-03-31 17:17:41.375209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6144 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:03:00.0, compute capability: 6.1)
2021-03-31 17:17:43.180250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-03-31 17:17:44.185292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
1 Physical GPUs, 1 Logical GPUs
Building the model ...
total 77874 examples ...
Epoch 1 Batch 50 Loss 2.9483
Epoch 1 Batch 100 Loss 2.6338
Epoch 1 Batch 150 Loss 2.6449
Epoch 1 Batch 200 Loss 2.5802
Epoch 1 Batch 250 Loss 2.6056
Epoch 1 Batch 300 Loss 2.5799
Epoch 1 Batch 350 Loss 2.5742
Epoch 1 Batch 400 Loss 2.5427
Epoch 1 Batch 450 Loss 2.5879
Epoch 1 Batch 500 Loss 2.5765
Epoch 1 Batch 550 Loss 2.4408
Epoch 1 Batch 600 Loss 2.5534
Epoch 1 Batch 650 Loss 2.4551
Epoch 1 Batch 700 Loss 2.4995
Epoch 1 Batch 750 Loss 2.4885
Epoch 1 Batch 800 Loss 2.5153
Epoch 1 Batch 850 Loss 2.4870
Epoch 1 Batch 900 Loss 2.4297
Epoch 1 Batch 950 Loss 2.4605
Epoch 1 Batch 1000 Loss 2.3935
Epoch 1 Batch 1050 Loss 2.3842
Epoch 1 Batch 1100 Loss 2.4612
Epoch 1 Batch 1150 Loss 2.3358
Epoch 1 Batch 1200 Loss 2.4321
Epoch 1 Batch 1250 Loss 2.3441
Epoch 1 Batch 1300 Loss 2.3842
Epoch 1 Batch 1350 Loss 2.2812
Epoch 1 Batch 1400 Loss 2.3029
Epoch 1 Batch 1450 Loss 2.2471
Epoch 1 Batch 1500 Loss 2.2147
Epoch 1 Batch 1550 Loss 2.2367
Epoch 1 Batch 1600 Loss 2.2255
Epoch 1 Batch 1650 Loss 2.1423
Epoch 1 Batch 1700 Loss 2.1795
Epoch 1 Batch 1750 Loss 2.1065
Epoch 1 Batch 1800 Loss 2.0894
Epoch 1 Batch 1850 Loss 2.0966
Epoch 1 Batch 1900 Loss 2.0452
Epoch 1 Batch 1950 Loss 2.0640
Epoch 1 Batch 2000 Loss 2.0706
Epoch 1 Batch 2050 Loss 1.9829
Epoch 1 Batch 2100 Loss 1.9668
Epoch 1 Batch 2150 Loss 1.9575
Epoch 1 Batch 2200 Loss 1.9236
Epoch 1 Batch 2250 Loss 1.9121
Epoch 1 Batch 2300 Loss 1.9208
Epoch 1 Batch 2350 Loss 1.8986
Epoch 1 Batch 2400 Loss 1.8601
Starting evaluate ...
Epoch 1 Loss 2.3075; val Loss 1.8692
Time taken for 1 epoch 1306.1800827980042 sec

Epoch 2 Batch 50 Loss 1.7308
Epoch 2 Batch 100 Loss 1.8296
Epoch 2 Batch 150 Loss 1.7789
Epoch 2 Batch 200 Loss 1.7748
Epoch 2 Batch 250 Loss 1.6980
Epoch 2 Batch 300 Loss 1.7580
Epoch 2 Batch 350 Loss 1.7395
Epoch 2 Batch 400 Loss 1.7190
Epoch 2 Batch 450 Loss 1.7221
Epoch 2 Batch 500 Loss 1.7238
Epoch 2 Batch 550 Loss 1.6896
Epoch 2 Batch 600 Loss 1.7454
Epoch 2 Batch 650 Loss 1.7192
Epoch 2 Batch 700 Loss 1.6788
Epoch 2 Batch 750 Loss 1.6785
Epoch 2 Batch 800 Loss 1.6567
Epoch 2 Batch 850 Loss 1.6575
Epoch 2 Batch 900 Loss 1.6418
Epoch 2 Batch 950 Loss 1.6489
Epoch 2 Batch 1000 Loss 1.6576
Epoch 2 Batch 1050 Loss 1.6622
Epoch 2 Batch 1100 Loss 1.6408
Epoch 2 Batch 1150 Loss 1.6551
Epoch 2 Batch 1200 Loss 1.6273
Epoch 2 Batch 1250 Loss 1.6040
Epoch 2 Batch 1300 Loss 1.6208
Epoch 2 Batch 1350 Loss 1.6145
Epoch 2 Batch 1400 Loss 1.6449
Epoch 2 Batch 1450 Loss 1.6023
Epoch 2 Batch 1500 Loss 1.6043
Epoch 2 Batch 1550 Loss 1.6020
Epoch 2 Batch 1600 Loss 1.5963
Epoch 2 Batch 1650 Loss 1.5742
Epoch 2 Batch 1700 Loss 1.5807
Epoch 2 Batch 1750 Loss 1.5788
Epoch 2 Batch 1800 Loss 1.6216
Epoch 2 Batch 1850 Loss 1.5968
Epoch 2 Batch 1900 Loss 1.5764
Epoch 2 Batch 1950 Loss 1.5575
Epoch 2 Batch 2000 Loss 1.5625
Epoch 2 Batch 2050 Loss 1.5705
Epoch 2 Batch 2100 Loss 1.5321
Epoch 2 Batch 2150 Loss 1.5126
Epoch 2 Batch 2200 Loss 1.5445
Epoch 2 Batch 2250 Loss 1.5218
Epoch 2 Batch 2300 Loss 1.5486
Epoch 2 Batch 2350 Loss 1.5592
Epoch 2 Batch 2400 Loss 1.5488
Saving checkpoint for epoch 2 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-1
Starting evaluate ...
Epoch 2 Loss 1.6380; val Loss 1.5644
Time taken for 1 epoch 1300.177936077118 sec

Epoch 3 Batch 50 Loss 1.4137
Epoch 3 Batch 100 Loss 1.4150
Epoch 3 Batch 150 Loss 1.4731
Epoch 3 Batch 200 Loss 1.4503
Epoch 3 Batch 250 Loss 1.4380
Epoch 3 Batch 300 Loss 1.4198
Epoch 3 Batch 350 Loss 1.4398
Epoch 3 Batch 400 Loss 1.4509
Epoch 3 Batch 450 Loss 1.4455
Epoch 3 Batch 500 Loss 1.4191
Epoch 3 Batch 550 Loss 1.4403
Epoch 3 Batch 600 Loss 1.3790
Epoch 3 Batch 650 Loss 1.4336
Epoch 3 Batch 700 Loss 1.4056
Epoch 3 Batch 750 Loss 1.4653
Epoch 3 Batch 800 Loss 1.4221
Epoch 3 Batch 850 Loss 1.3950
Epoch 3 Batch 900 Loss 1.3985
Epoch 3 Batch 950 Loss 1.3820
Epoch 3 Batch 1000 Loss 1.3845
Epoch 3 Batch 1050 Loss 1.4152
Epoch 3 Batch 1100 Loss 1.3934
Epoch 3 Batch 1150 Loss 1.4165
Epoch 3 Batch 1200 Loss 1.4242
Epoch 3 Batch 1250 Loss 1.4571
Epoch 3 Batch 1300 Loss 1.4229
Epoch 3 Batch 1350 Loss 1.4028
Epoch 3 Batch 1400 Loss 1.4050
Epoch 3 Batch 1450 Loss 1.3805
Epoch 3 Batch 1500 Loss 1.4284
Epoch 3 Batch 1550 Loss 1.4051
Epoch 3 Batch 1600 Loss 1.4361
Epoch 3 Batch 1650 Loss 1.3887
Epoch 3 Batch 1700 Loss 1.4142
Epoch 3 Batch 1750 Loss 1.4224
Epoch 3 Batch 1800 Loss 1.3688
Epoch 3 Batch 1850 Loss 1.4036
Epoch 3 Batch 1900 Loss 1.3789
Epoch 3 Batch 1950 Loss 1.3849
Epoch 3 Batch 2000 Loss 1.3940
Epoch 3 Batch 2050 Loss 1.4369
Epoch 3 Batch 2100 Loss 1.3752
Epoch 3 Batch 2150 Loss 1.3739
Epoch 3 Batch 2200 Loss 1.3408
Epoch 3 Batch 2250 Loss 1.4070
Epoch 3 Batch 2300 Loss 1.3995
Epoch 3 Batch 2350 Loss 1.4025
Epoch 3 Batch 2400 Loss 1.3371
Starting evaluate ...
Epoch 3 Loss 1.4104; val Loss 1.4666
Time taken for 1 epoch 1300.7436451911926 sec

Epoch 4 Batch 50 Loss 1.2636
Epoch 4 Batch 100 Loss 1.2969
Epoch 4 Batch 150 Loss 1.2642
Epoch 4 Batch 200 Loss 1.2879
Epoch 4 Batch 250 Loss 1.3081
Epoch 4 Batch 300 Loss 1.2566
Epoch 4 Batch 350 Loss 1.2598
Epoch 4 Batch 400 Loss 1.3021
Epoch 4 Batch 450 Loss 1.2742
Epoch 4 Batch 500 Loss 1.2623
Epoch 4 Batch 550 Loss 1.3316
Epoch 4 Batch 600 Loss 1.2918
Epoch 4 Batch 650 Loss 1.2516
Epoch 4 Batch 700 Loss 1.3261
Epoch 4 Batch 750 Loss 1.2991
Epoch 4 Batch 800 Loss 1.3160
Epoch 4 Batch 850 Loss 1.3011
Epoch 4 Batch 900 Loss 1.2612
Epoch 4 Batch 950 Loss 1.3178
Epoch 4 Batch 1000 Loss 1.2908
Epoch 4 Batch 1050 Loss 1.2672
Epoch 4 Batch 1100 Loss 1.2865
Epoch 4 Batch 1150 Loss 1.2961
Epoch 4 Batch 1200 Loss 1.2879
Epoch 4 Batch 1250 Loss 1.2755
Epoch 4 Batch 1300 Loss 1.2734
Epoch 4 Batch 1350 Loss 1.3087
Epoch 4 Batch 1400 Loss 1.2770
Epoch 4 Batch 1450 Loss 1.2685
Epoch 4 Batch 1500 Loss 1.2961
Epoch 4 Batch 1550 Loss 1.2745
Epoch 4 Batch 1600 Loss 1.3400
Epoch 4 Batch 1650 Loss 1.2613
Epoch 4 Batch 1700 Loss 1.2903
Epoch 4 Batch 1750 Loss 1.2816
Epoch 4 Batch 1800 Loss 1.2854
Epoch 4 Batch 1850 Loss 1.3070
Epoch 4 Batch 1900 Loss 1.2979
Epoch 4 Batch 1950 Loss 1.3016
Epoch 4 Batch 2000 Loss 1.3173
Epoch 4 Batch 2050 Loss 1.2526
Epoch 4 Batch 2100 Loss 1.2410
Epoch 4 Batch 2150 Loss 1.3035
Epoch 4 Batch 2200 Loss 1.2262
Epoch 4 Batch 2250 Loss 1.3055
Epoch 4 Batch 2300 Loss 1.2552
Epoch 4 Batch 2350 Loss 1.2745
Epoch 4 Batch 2400 Loss 1.2738
Saving checkpoint for epoch 4 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-2
Starting evaluate ...
Epoch 4 Loss 1.2852; val Loss 1.4173
Time taken for 1 epoch 1294.3068025112152 sec

Epoch 5 Batch 50 Loss 1.1666
Epoch 5 Batch 100 Loss 1.1516
Epoch 5 Batch 150 Loss 1.1801
Epoch 5 Batch 200 Loss 1.1930
Epoch 5 Batch 250 Loss 1.2259
Epoch 5 Batch 300 Loss 1.2248
Epoch 5 Batch 350 Loss 1.1925
Epoch 5 Batch 400 Loss 1.2002
Epoch 5 Batch 450 Loss 1.2002
Epoch 5 Batch 500 Loss 1.2098
Epoch 5 Batch 550 Loss 1.1826
Epoch 5 Batch 600 Loss 1.2207
Epoch 5 Batch 650 Loss 1.1556
Epoch 5 Batch 700 Loss 1.1850
Epoch 5 Batch 750 Loss 1.2092
Epoch 5 Batch 800 Loss 1.1943
Epoch 5 Batch 850 Loss 1.2140
Epoch 5 Batch 900 Loss 1.2541
Epoch 5 Batch 950 Loss 1.2062
Epoch 5 Batch 1000 Loss 1.1925
Epoch 5 Batch 1050 Loss 1.2004
Epoch 5 Batch 1100 Loss 1.1995
Epoch 5 Batch 1150 Loss 1.1893
Epoch 5 Batch 1200 Loss 1.2074
Epoch 5 Batch 1250 Loss 1.2148
Epoch 5 Batch 1300 Loss 1.1836
Epoch 5 Batch 1350 Loss 1.1639
Epoch 5 Batch 1400 Loss 1.1941
Epoch 5 Batch 1450 Loss 1.2273
Epoch 5 Batch 1500 Loss 1.1976
Epoch 5 Batch 1550 Loss 1.1957
Epoch 5 Batch 1600 Loss 1.2263
Epoch 5 Batch 1650 Loss 1.1998
Epoch 5 Batch 1700 Loss 1.1981
Epoch 5 Batch 1750 Loss 1.1848
Epoch 5 Batch 1800 Loss 1.1794
Epoch 5 Batch 1850 Loss 1.2214
Epoch 5 Batch 1900 Loss 1.1988
Epoch 5 Batch 1950 Loss 1.2188
Epoch 5 Batch 2000 Loss 1.1988
Epoch 5 Batch 2050 Loss 1.1984
Epoch 5 Batch 2100 Loss 1.1913
Epoch 5 Batch 2150 Loss 1.1929
Epoch 5 Batch 2200 Loss 1.2078
Epoch 5 Batch 2250 Loss 1.2111
Epoch 5 Batch 2300 Loss 1.1916
Epoch 5 Batch 2350 Loss 1.2000
Epoch 5 Batch 2400 Loss 1.2341
Starting evaluate ...
Epoch 5 Loss 1.2000; val Loss 1.4048
Time taken for 1 epoch 1288.4013059139252 sec

Epoch 6 Batch 50 Loss 1.0848
Epoch 6 Batch 100 Loss 1.1290
Epoch 6 Batch 150 Loss 1.1431
Epoch 6 Batch 200 Loss 1.1220
Epoch 6 Batch 250 Loss 1.0830
Epoch 6 Batch 300 Loss 1.0985
Epoch 6 Batch 350 Loss 1.1078
Epoch 6 Batch 400 Loss 1.1082
Epoch 6 Batch 450 Loss 1.1196
Epoch 6 Batch 500 Loss 1.1467
Epoch 6 Batch 550 Loss 1.1170
Epoch 6 Batch 600 Loss 1.1448
Epoch 6 Batch 650 Loss 1.1564
Epoch 6 Batch 700 Loss 1.1289
Epoch 6 Batch 750 Loss 1.1317
Epoch 6 Batch 800 Loss 1.1200
Epoch 6 Batch 850 Loss 1.1316
Epoch 6 Batch 900 Loss 1.1740
Epoch 6 Batch 950 Loss 1.1643
Epoch 6 Batch 1000 Loss 1.1357
Epoch 6 Batch 1050 Loss 1.1368
Epoch 6 Batch 1100 Loss 1.1312
Epoch 6 Batch 1150 Loss 1.1274
Epoch 6 Batch 1200 Loss 1.1251
Epoch 6 Batch 1250 Loss 1.1520
Epoch 6 Batch 1300 Loss 1.1375
Epoch 6 Batch 1350 Loss 1.1142
Epoch 6 Batch 1400 Loss 1.1456
Epoch 6 Batch 1450 Loss 1.1527
Epoch 6 Batch 1500 Loss 1.1427
Epoch 6 Batch 1550 Loss 1.1552
Epoch 6 Batch 1600 Loss 1.1618
Epoch 6 Batch 1650 Loss 1.1412
Epoch 6 Batch 1700 Loss 1.1743
Epoch 6 Batch 1750 Loss 1.1570
Epoch 6 Batch 1800 Loss 1.1018
Epoch 6 Batch 1850 Loss 1.1367
Epoch 6 Batch 1900 Loss 1.1353
Epoch 6 Batch 1950 Loss 1.1221
Epoch 6 Batch 2000 Loss 1.1696
Epoch 6 Batch 2050 Loss 1.1814
Epoch 6 Batch 2100 Loss 1.1238
Epoch 6 Batch 2150 Loss 1.1443
Epoch 6 Batch 2200 Loss 1.1405
Epoch 6 Batch 2250 Loss 1.1679
Epoch 6 Batch 2300 Loss 1.1650
Epoch 6 Batch 2350 Loss 1.1396
Epoch 6 Batch 2400 Loss 1.1464
Saving checkpoint for epoch 6 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-3
Starting evaluate ...
Epoch 6 Loss 1.1374; val Loss 1.3940
Time taken for 1 epoch 1294.7367994785309 sec

Epoch 7 Batch 50 Loss 1.0616
Epoch 7 Batch 100 Loss 1.0419
Epoch 7 Batch 150 Loss 1.0600
Epoch 7 Batch 200 Loss 1.0560
Epoch 7 Batch 250 Loss 1.0659
Epoch 7 Batch 300 Loss 1.0752
Epoch 7 Batch 350 Loss 1.0961
Epoch 7 Batch 400 Loss 1.0548
Epoch 7 Batch 450 Loss 1.0866
Epoch 7 Batch 500 Loss 1.0433
Epoch 7 Batch 550 Loss 1.0631
Epoch 7 Batch 600 Loss 1.0705
Epoch 7 Batch 650 Loss 1.0793
Epoch 7 Batch 700 Loss 1.0822
Epoch 7 Batch 750 Loss 1.0854
Epoch 7 Batch 800 Loss 1.0740
Epoch 7 Batch 850 Loss 1.0880
Epoch 7 Batch 900 Loss 1.1017
Epoch 7 Batch 950 Loss 1.0684
Epoch 7 Batch 1000 Loss 1.0855
Epoch 7 Batch 1050 Loss 1.0748
Epoch 7 Batch 1100 Loss 1.0954
Epoch 7 Batch 1150 Loss 1.0293
Epoch 7 Batch 1200 Loss 1.1133
Epoch 7 Batch 1250 Loss 1.0799
Epoch 7 Batch 1300 Loss 1.1289
Epoch 7 Batch 1350 Loss 1.0837
Epoch 7 Batch 1400 Loss 1.0856
Epoch 7 Batch 1450 Loss 1.1257
Epoch 7 Batch 1500 Loss 1.0899
Epoch 7 Batch 1550 Loss 1.1051
Epoch 7 Batch 1600 Loss 1.0799
Epoch 7 Batch 1650 Loss 1.0667
Epoch 7 Batch 1700 Loss 1.0755
Epoch 7 Batch 1750 Loss 1.1053
Epoch 7 Batch 1800 Loss 1.1105
Epoch 7 Batch 1850 Loss 1.1033
Epoch 7 Batch 1900 Loss 1.1153
Epoch 7 Batch 1950 Loss 1.0905
Epoch 7 Batch 2000 Loss 1.1308
Epoch 7 Batch 2050 Loss 1.0970
Epoch 7 Batch 2100 Loss 1.1141
Epoch 7 Batch 2150 Loss 1.1097
Epoch 7 Batch 2200 Loss 1.0670
Epoch 7 Batch 2250 Loss 1.1142
Epoch 7 Batch 2300 Loss 1.0980
Epoch 7 Batch 2350 Loss 1.0917
Epoch 7 Batch 2400 Loss 1.1265
Starting evaluate ...
Epoch 7 Loss 1.0863; val Loss 1.3945
Time taken for 1 epoch 1298.032957792282 sec

Epoch 8 Batch 50 Loss 0.9931
Epoch 8 Batch 100 Loss 1.0081
Epoch 8 Batch 150 Loss 1.0110
Epoch 8 Batch 200 Loss 1.0152
Epoch 8 Batch 250 Loss 0.9983
Epoch 8 Batch 300 Loss 1.0069
Epoch 8 Batch 350 Loss 1.0404
Epoch 8 Batch 400 Loss 1.0109
Epoch 8 Batch 450 Loss 1.0138
Epoch 8 Batch 500 Loss 1.0373
Epoch 8 Batch 550 Loss 1.0401
Epoch 8 Batch 600 Loss 1.0410
Epoch 8 Batch 650 Loss 1.0507
Epoch 8 Batch 700 Loss 1.0504
Epoch 8 Batch 750 Loss 1.0493
Epoch 8 Batch 800 Loss 1.0222
Epoch 8 Batch 850 Loss 1.0443
Epoch 8 Batch 900 Loss 1.0381
Epoch 8 Batch 950 Loss 1.0357
Epoch 8 Batch 1000 Loss 1.0306
Epoch 8 Batch 1050 Loss 1.0324
Epoch 8 Batch 1100 Loss 1.0551
Epoch 8 Batch 1150 Loss 1.0671
Epoch 8 Batch 1200 Loss 1.0537
Epoch 8 Batch 1250 Loss 1.0236
Epoch 8 Batch 1300 Loss 1.0630
Epoch 8 Batch 1350 Loss 1.0339
Epoch 8 Batch 1400 Loss 1.0450
Epoch 8 Batch 1450 Loss 1.0560
Epoch 8 Batch 1500 Loss 1.0499
Epoch 8 Batch 1550 Loss 1.0739
Epoch 8 Batch 1600 Loss 1.0752
Epoch 8 Batch 1650 Loss 1.0705
Epoch 8 Batch 1700 Loss 1.0735
Epoch 8 Batch 1750 Loss 1.0418
Epoch 8 Batch 1800 Loss 1.0818
Epoch 8 Batch 1850 Loss 1.0911
Epoch 8 Batch 1900 Loss 1.0624
Epoch 8 Batch 1950 Loss 1.0515
Epoch 8 Batch 2000 Loss 1.0996
Epoch 8 Batch 2050 Loss 1.0483
Epoch 8 Batch 2100 Loss 1.0818
Epoch 8 Batch 2150 Loss 1.0480
Epoch 8 Batch 2200 Loss 1.0662
Epoch 8 Batch 2250 Loss 1.0370
Epoch 8 Batch 2300 Loss 1.0711
Epoch 8 Batch 2350 Loss 1.0436
Epoch 8 Batch 2400 Loss 1.0643
Saving checkpoint for epoch 8 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-4
Starting evaluate ...
Epoch 8 Loss 1.0460; val Loss 1.4000
Time taken for 1 epoch 1291.3049149513245 sec

Epoch 9 Batch 50 Loss 0.9571
Epoch 9 Batch 100 Loss 0.9553
Epoch 9 Batch 150 Loss 0.9731
Epoch 9 Batch 200 Loss 0.9621
Epoch 9 Batch 250 Loss 0.9805
Epoch 9 Batch 300 Loss 0.9684
Epoch 9 Batch 350 Loss 1.0069
Epoch 9 Batch 400 Loss 0.9751
Epoch 9 Batch 450 Loss 0.9995
Epoch 9 Batch 500 Loss 1.0090
Epoch 9 Batch 550 Loss 1.0026
Epoch 9 Batch 600 Loss 0.9643
Epoch 9 Batch 650 Loss 0.9835
Epoch 9 Batch 700 Loss 0.9975
Epoch 9 Batch 750 Loss 1.0176
Epoch 9 Batch 800 Loss 1.0030
Epoch 9 Batch 850 Loss 0.9786
Epoch 9 Batch 900 Loss 1.0211
Epoch 9 Batch 950 Loss 1.0004
Epoch 9 Batch 1000 Loss 1.0221
Epoch 9 Batch 1050 Loss 1.0385
Epoch 9 Batch 1100 Loss 1.0142
Epoch 9 Batch 1150 Loss 1.0144
Epoch 9 Batch 1200 Loss 1.0074
Epoch 9 Batch 1250 Loss 1.0317
Epoch 9 Batch 1300 Loss 1.0191
Epoch 9 Batch 1350 Loss 0.9940
Epoch 9 Batch 1400 Loss 1.0374
Epoch 9 Batch 1450 Loss 1.0244
Epoch 9 Batch 1500 Loss 1.0321
Epoch 9 Batch 1550 Loss 0.9914
Epoch 9 Batch 1600 Loss 1.0193
Epoch 9 Batch 1650 Loss 0.9954
Epoch 9 Batch 1700 Loss 1.0250
Epoch 9 Batch 1750 Loss 0.9836
Epoch 9 Batch 1800 Loss 1.0444
Epoch 9 Batch 1850 Loss 1.0198
Epoch 9 Batch 1900 Loss 1.0718
Epoch 9 Batch 1950 Loss 1.0482
Epoch 9 Batch 2000 Loss 1.0163
Epoch 9 Batch 2050 Loss 1.0495
Epoch 9 Batch 2100 Loss 1.0707
Epoch 9 Batch 2150 Loss 1.0045
Epoch 9 Batch 2200 Loss 1.0367
Epoch 9 Batch 2250 Loss 1.0398
Epoch 9 Batch 2300 Loss 1.0660
Epoch 9 Batch 2350 Loss 1.0253
Epoch 9 Batch 2400 Loss 1.0172
Starting evaluate ...
Epoch 9 Loss 1.0118; val Loss 1.4117
Time taken for 1 epoch 1283.267729997635 sec

Epoch 10 Batch 50 Loss 0.9314
Epoch 10 Batch 100 Loss 0.9121
Epoch 10 Batch 150 Loss 0.9208
Epoch 10 Batch 200 Loss 0.9469
Epoch 10 Batch 250 Loss 0.9299
Epoch 10 Batch 300 Loss 0.9549
Epoch 10 Batch 350 Loss 0.9571
Epoch 10 Batch 400 Loss 0.9570
Epoch 10 Batch 450 Loss 0.9451
Epoch 10 Batch 500 Loss 0.9868
Epoch 10 Batch 550 Loss 0.9554
Epoch 10 Batch 600 Loss 1.0013
Epoch 10 Batch 650 Loss 0.9534
Epoch 10 Batch 700 Loss 0.9745
Epoch 10 Batch 750 Loss 0.9746
Epoch 10 Batch 800 Loss 0.9584
Epoch 10 Batch 850 Loss 0.9543
Epoch 10 Batch 900 Loss 0.9551
Epoch 10 Batch 950 Loss 0.9786
Epoch 10 Batch 1000 Loss 0.9958
Epoch 10 Batch 1050 Loss 0.9781
Epoch 10 Batch 1100 Loss 0.9772
Epoch 10 Batch 1150 Loss 1.0034
Epoch 10 Batch 1200 Loss 0.9327
Epoch 10 Batch 1250 Loss 1.0059
Epoch 10 Batch 1300 Loss 0.9710
Epoch 10 Batch 1350 Loss 1.0080
Epoch 10 Batch 1400 Loss 0.9663
Epoch 10 Batch 1450 Loss 0.9784
Epoch 10 Batch 1500 Loss 0.9861
Epoch 10 Batch 1550 Loss 1.0234
Epoch 10 Batch 1600 Loss 0.9631
Epoch 10 Batch 1650 Loss 1.0400
Epoch 10 Batch 1700 Loss 0.9975
Epoch 10 Batch 1750 Loss 0.9986
Epoch 10 Batch 1800 Loss 1.0223
Epoch 10 Batch 1850 Loss 0.9932
Epoch 10 Batch 1900 Loss 1.0014
Epoch 10 Batch 1950 Loss 1.0163
Epoch 10 Batch 2000 Loss 1.0046
Epoch 10 Batch 2050 Loss 1.0064
Epoch 10 Batch 2100 Loss 0.9907
Epoch 10 Batch 2150 Loss 1.0381
Epoch 10 Batch 2200 Loss 1.0391
Epoch 10 Batch 2250 Loss 1.0272
Epoch 10 Batch 2300 Loss 0.9808
Epoch 10 Batch 2350 Loss 1.0048
Epoch 10 Batch 2400 Loss 1.0459
Saving checkpoint for epoch 10 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-5
Starting evaluate ...
Epoch 10 Loss 0.9824; val Loss 1.4259
Time taken for 1 epoch 1240.3792502880096 sec

Epoch 11 Batch 50 Loss 0.8949
Epoch 11 Batch 100 Loss 0.9042
Epoch 11 Batch 150 Loss 0.9208
Epoch 11 Batch 200 Loss 0.9248
Epoch 11 Batch 250 Loss 0.9189
Epoch 11 Batch 300 Loss 0.9194
Epoch 11 Batch 350 Loss 0.9404
Epoch 11 Batch 400 Loss 0.9217
Epoch 11 Batch 450 Loss 0.9305
Epoch 11 Batch 500 Loss 0.9364
Epoch 11 Batch 550 Loss 0.9616
Epoch 11 Batch 600 Loss 0.9168
Epoch 11 Batch 650 Loss 0.9616
Epoch 11 Batch 700 Loss 0.9594
Epoch 11 Batch 750 Loss 0.9029
Epoch 11 Batch 800 Loss 0.9234
Epoch 11 Batch 850 Loss 0.9173
Epoch 11 Batch 900 Loss 0.9630
Epoch 11 Batch 950 Loss 0.9598
Epoch 11 Batch 1000 Loss 0.9356
Epoch 11 Batch 1050 Loss 0.9635
Epoch 11 Batch 1100 Loss 0.9706
Epoch 11 Batch 1150 Loss 0.9637
Epoch 11 Batch 1200 Loss 0.9985
Epoch 11 Batch 1250 Loss 0.9676
Epoch 11 Batch 1300 Loss 0.9833
Epoch 11 Batch 1350 Loss 0.9524
Epoch 11 Batch 1400 Loss 0.9534
Epoch 11 Batch 1450 Loss 0.9839
Epoch 11 Batch 1500 Loss 0.9696
Epoch 11 Batch 1550 Loss 0.9475
Epoch 11 Batch 1600 Loss 0.9500
Epoch 11 Batch 1650 Loss 0.9744
Epoch 11 Batch 1700 Loss 0.9885
Epoch 11 Batch 1750 Loss 0.9761
Epoch 11 Batch 1800 Loss 0.9910
Epoch 11 Batch 1850 Loss 0.9819
Epoch 11 Batch 1900 Loss 0.9982
Epoch 11 Batch 1950 Loss 0.9827
Epoch 11 Batch 2000 Loss 0.9715
Epoch 11 Batch 2050 Loss 0.9676
Epoch 11 Batch 2100 Loss 0.9496
Epoch 11 Batch 2150 Loss 0.9926
Epoch 11 Batch 2200 Loss 0.9858
Epoch 11 Batch 2250 Loss 0.9723
Epoch 11 Batch 2300 Loss 0.9478
Epoch 11 Batch 2350 Loss 1.0158
Epoch 11 Batch 2400 Loss 0.9870
Starting evaluate ...
Epoch 11 Loss 0.9564; val Loss 1.4374
Time taken for 1 epoch 1228.9753229618073 sec

Epoch 12 Batch 50 Loss 0.8827
Epoch 12 Batch 100 Loss 0.9168
Epoch 12 Batch 150 Loss 0.8941
Epoch 12 Batch 200 Loss 0.8922
Epoch 12 Batch 250 Loss 0.9074
Epoch 12 Batch 300 Loss 0.8844
Epoch 12 Batch 350 Loss 0.8727
Epoch 12 Batch 400 Loss 0.9196
Epoch 12 Batch 450 Loss 0.9268
Epoch 12 Batch 500 Loss 0.8949
Epoch 12 Batch 550 Loss 0.9017
Epoch 12 Batch 600 Loss 0.9380
Epoch 12 Batch 650 Loss 0.9437
Epoch 12 Batch 700 Loss 0.9357
Epoch 12 Batch 750 Loss 0.9213
Epoch 12 Batch 800 Loss 0.9042
Epoch 12 Batch 850 Loss 0.9067
Epoch 12 Batch 900 Loss 0.9416
Epoch 12 Batch 950 Loss 0.9373
Epoch 12 Batch 1000 Loss 0.9506
Epoch 12 Batch 1050 Loss 0.9563
Epoch 12 Batch 1100 Loss 0.9138
Epoch 12 Batch 1150 Loss 0.9219
Epoch 12 Batch 1200 Loss 0.9358
Epoch 12 Batch 1250 Loss 0.9454
Epoch 12 Batch 1300 Loss 0.9593
Epoch 12 Batch 1350 Loss 0.9360
Epoch 12 Batch 1400 Loss 0.9356
Epoch 12 Batch 1450 Loss 0.9564
Epoch 12 Batch 1500 Loss 0.9435
Epoch 12 Batch 1550 Loss 0.9258
Epoch 12 Batch 1600 Loss 0.9312
Epoch 12 Batch 1650 Loss 0.9477
Epoch 12 Batch 1700 Loss 0.9656
Epoch 12 Batch 1750 Loss 0.9030
Epoch 12 Batch 1800 Loss 0.9534
Epoch 12 Batch 1850 Loss 0.9634
Epoch 12 Batch 1900 Loss 0.9352
Epoch 12 Batch 1950 Loss 0.9246
Epoch 12 Batch 2000 Loss 0.9382
Epoch 12 Batch 2050 Loss 0.9568
Epoch 12 Batch 2100 Loss 0.9659
Epoch 12 Batch 2150 Loss 0.9338
Epoch 12 Batch 2200 Loss 0.9651
Epoch 12 Batch 2250 Loss 0.9741
Epoch 12 Batch 2300 Loss 0.9826
Epoch 12 Batch 2350 Loss 0.9670
Epoch 12 Batch 2400 Loss 0.9662
Saving checkpoint for epoch 12 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-6
Starting evaluate ...
Epoch 12 Loss 0.9332; val Loss 1.4550
Time taken for 1 epoch 1239.566437959671 sec

Epoch 13 Batch 50 Loss 0.8649
Epoch 13 Batch 100 Loss 0.8645
Epoch 13 Batch 150 Loss 0.8971
Epoch 13 Batch 200 Loss 0.8600
Epoch 13 Batch 250 Loss 0.8799
Epoch 13 Batch 300 Loss 0.8656
Epoch 13 Batch 350 Loss 0.8909
Epoch 13 Batch 400 Loss 0.8823
Epoch 13 Batch 450 Loss 0.8911
Epoch 13 Batch 500 Loss 0.8852
Epoch 13 Batch 550 Loss 0.8710
Epoch 13 Batch 600 Loss 0.9053
Epoch 13 Batch 650 Loss 0.8805
Epoch 13 Batch 700 Loss 0.8795
Epoch 13 Batch 750 Loss 0.9207
Epoch 13 Batch 800 Loss 0.8999
Epoch 13 Batch 850 Loss 0.8577
Epoch 13 Batch 900 Loss 0.9066
Epoch 13 Batch 950 Loss 0.8940
Epoch 13 Batch 1000 Loss 0.9316
Epoch 13 Batch 1050 Loss 0.9169
Epoch 13 Batch 1100 Loss 0.8965
Epoch 13 Batch 1150 Loss 0.9211
Epoch 13 Batch 1200 Loss 0.8883
Epoch 13 Batch 1250 Loss 0.9140
Epoch 13 Batch 1300 Loss 0.9577
Epoch 13 Batch 1350 Loss 0.8967
Epoch 13 Batch 1400 Loss 0.9078
Epoch 13 Batch 1450 Loss 0.9294
Epoch 13 Batch 1500 Loss 0.9452
Epoch 13 Batch 1550 Loss 0.9244
Epoch 13 Batch 1600 Loss 0.9272
Epoch 13 Batch 1650 Loss 0.9150
Epoch 13 Batch 1700 Loss 0.9477
Epoch 13 Batch 1750 Loss 0.9363
Epoch 13 Batch 1800 Loss 0.9394
Epoch 13 Batch 1850 Loss 0.9443
Epoch 13 Batch 1900 Loss 0.9124
Epoch 13 Batch 1950 Loss 0.9373
Epoch 13 Batch 2000 Loss 0.9626
Epoch 13 Batch 2050 Loss 0.9252
Epoch 13 Batch 2100 Loss 0.9526
Epoch 13 Batch 2150 Loss 0.9446
Epoch 13 Batch 2200 Loss 0.9494
Epoch 13 Batch 2250 Loss 0.9268
Epoch 13 Batch 2300 Loss 0.9639
Epoch 13 Batch 2350 Loss 0.9388
Epoch 13 Batch 2400 Loss 0.8961
Starting evaluate ...
Epoch 13 Loss 0.9117; val Loss 1.4656
Time taken for 1 epoch 1239.6903157234192 sec

Epoch 14 Batch 50 Loss 0.8452
Epoch 14 Batch 100 Loss 0.8551
Epoch 14 Batch 150 Loss 0.8430
Epoch 14 Batch 200 Loss 0.8381
Epoch 14 Batch 250 Loss 0.8783
Epoch 14 Batch 300 Loss 0.8616
Epoch 14 Batch 350 Loss 0.8365
Epoch 14 Batch 400 Loss 0.8736
Epoch 14 Batch 450 Loss 0.8893
Epoch 14 Batch 500 Loss 0.8723
Epoch 14 Batch 550 Loss 0.8619
Epoch 14 Batch 600 Loss 0.8720
Epoch 14 Batch 650 Loss 0.8546
Epoch 14 Batch 700 Loss 0.8835
Epoch 14 Batch 750 Loss 0.8905
Epoch 14 Batch 800 Loss 0.8587
Epoch 14 Batch 850 Loss 0.8698
Epoch 14 Batch 900 Loss 0.9308
Epoch 14 Batch 950 Loss 0.9018
Epoch 14 Batch 1000 Loss 0.9101
Epoch 14 Batch 1050 Loss 0.9301
Epoch 14 Batch 1100 Loss 0.9025
Epoch 14 Batch 1150 Loss 0.8898
Epoch 14 Batch 1200 Loss 0.9213
Epoch 14 Batch 1250 Loss 0.9443
Epoch 14 Batch 1300 Loss 0.8984
Epoch 14 Batch 1350 Loss 0.8783
Epoch 14 Batch 1400 Loss 0.8722
Epoch 14 Batch 1450 Loss 0.8955
Epoch 14 Batch 1500 Loss 0.8946
Epoch 14 Batch 1550 Loss 0.9172
Epoch 14 Batch 1600 Loss 0.8916
Epoch 14 Batch 1650 Loss 0.9185
Epoch 14 Batch 1700 Loss 0.8900
Epoch 14 Batch 1750 Loss 0.9068
Epoch 14 Batch 1800 Loss 0.9156
Epoch 14 Batch 1850 Loss 0.8985
Epoch 14 Batch 1900 Loss 0.9219
Epoch 14 Batch 1950 Loss 0.8952
Epoch 14 Batch 2000 Loss 0.9209
Epoch 14 Batch 2050 Loss 0.9089
Epoch 14 Batch 2100 Loss 0.8897
Epoch 14 Batch 2150 Loss 0.9317
Epoch 14 Batch 2200 Loss 0.9497
Epoch 14 Batch 2250 Loss 0.9363
Epoch 14 Batch 2300 Loss 0.9093
Epoch 14 Batch 2350 Loss 0.9241
Epoch 14 Batch 2400 Loss 0.9236
Saving checkpoint for epoch 14 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-7
Starting evaluate ...
Epoch 14 Loss 0.8947; val Loss 1.4788
Time taken for 1 epoch 1238.1188399791718 sec

Epoch 15 Batch 50 Loss 0.8380
Epoch 15 Batch 100 Loss 0.8140
Epoch 15 Batch 150 Loss 0.8238
Epoch 15 Batch 200 Loss 0.7974
Epoch 15 Batch 250 Loss 0.8258
Epoch 15 Batch 300 Loss 0.8612
Epoch 15 Batch 350 Loss 0.8573
Epoch 15 Batch 400 Loss 0.8581
Epoch 15 Batch 450 Loss 0.8810
Epoch 15 Batch 500 Loss 0.8390
Epoch 15 Batch 550 Loss 0.8987
Epoch 15 Batch 600 Loss 0.8539
Epoch 15 Batch 650 Loss 0.8738
Epoch 15 Batch 700 Loss 0.8301
Epoch 15 Batch 750 Loss 0.8429
Epoch 15 Batch 800 Loss 0.8467
Epoch 15 Batch 850 Loss 0.8862
Epoch 15 Batch 900 Loss 0.8982
Epoch 15 Batch 950 Loss 0.8539
Epoch 15 Batch 1000 Loss 0.8940
Epoch 15 Batch 1050 Loss 0.8860
Epoch 15 Batch 1100 Loss 0.8495
Epoch 15 Batch 1150 Loss 0.8732
Epoch 15 Batch 1200 Loss 0.8706
Epoch 15 Batch 1250 Loss 0.8957
Epoch 15 Batch 1300 Loss 0.8890
Epoch 15 Batch 1350 Loss 0.9057
Epoch 15 Batch 1400 Loss 0.8687
Epoch 15 Batch 1450 Loss 0.8863
Epoch 15 Batch 1500 Loss 0.8857
Epoch 15 Batch 1550 Loss 0.8633
Epoch 15 Batch 1600 Loss 0.8752
Epoch 15 Batch 1650 Loss 0.9169
Epoch 15 Batch 1700 Loss 0.8934
Epoch 15 Batch 1750 Loss 0.8981
Epoch 15 Batch 1800 Loss 0.8937
Epoch 15 Batch 1850 Loss 0.8862
Epoch 15 Batch 1900 Loss 0.9185
Epoch 15 Batch 1950 Loss 0.8642
Epoch 15 Batch 2000 Loss 0.9071
Epoch 15 Batch 2050 Loss 0.9132
Epoch 15 Batch 2100 Loss 0.9104
Epoch 15 Batch 2150 Loss 0.9116
Epoch 15 Batch 2200 Loss 0.9098
Epoch 15 Batch 2250 Loss 0.9058
Epoch 15 Batch 2300 Loss 0.9053
Epoch 15 Batch 2350 Loss 0.9200
Epoch 15 Batch 2400 Loss 0.8953
Starting evaluate ...
Epoch 15 Loss 0.8770; val Loss 1.4921
Time taken for 1 epoch 1248.5153992176056 sec

Epoch 16 Batch 50 Loss 0.8285
Epoch 16 Batch 100 Loss 0.8166
Epoch 16 Batch 150 Loss 0.8201
Epoch 16 Batch 200 Loss 0.8343
Epoch 16 Batch 250 Loss 0.8291
Epoch 16 Batch 300 Loss 0.8441
Epoch 16 Batch 350 Loss 0.8455
Epoch 16 Batch 400 Loss 0.8285
Epoch 16 Batch 450 Loss 0.8436
Epoch 16 Batch 500 Loss 0.8356
Epoch 16 Batch 550 Loss 0.8270
Epoch 16 Batch 600 Loss 0.8405
Epoch 16 Batch 650 Loss 0.8282
Epoch 16 Batch 700 Loss 0.8183
Epoch 16 Batch 750 Loss 0.8540
Epoch 16 Batch 800 Loss 0.8510
Epoch 16 Batch 850 Loss 0.8434
Epoch 16 Batch 900 Loss 0.8566
Epoch 16 Batch 950 Loss 0.8969
Epoch 16 Batch 1000 Loss 0.8588
Epoch 16 Batch 1050 Loss 0.8152
Epoch 16 Batch 1100 Loss 0.8508
Epoch 16 Batch 1150 Loss 0.8730
Epoch 16 Batch 1200 Loss 0.8784
Epoch 16 Batch 1250 Loss 0.8450
Epoch 16 Batch 1300 Loss 0.8643
Epoch 16 Batch 1350 Loss 0.8753
Epoch 16 Batch 1400 Loss 0.8639
Epoch 16 Batch 1450 Loss 0.8668
Epoch 16 Batch 1500 Loss 0.8778
Epoch 16 Batch 1550 Loss 0.8860
Epoch 16 Batch 1600 Loss 0.8719
Epoch 16 Batch 1650 Loss 0.8443
Epoch 16 Batch 1700 Loss 0.8486
Epoch 16 Batch 1750 Loss 0.8728
Epoch 16 Batch 1800 Loss 0.8680
Epoch 16 Batch 1850 Loss 0.8786
Epoch 16 Batch 1900 Loss 0.8998
Epoch 16 Batch 1950 Loss 0.8908
Epoch 16 Batch 2000 Loss 0.9176
Epoch 16 Batch 2050 Loss 0.8914
Epoch 16 Batch 2100 Loss 0.8918
Epoch 16 Batch 2150 Loss 0.8987
Epoch 16 Batch 2200 Loss 0.8989
Epoch 16 Batch 2250 Loss 0.8830
Epoch 16 Batch 2300 Loss 0.9092
Epoch 16 Batch 2350 Loss 0.8827
Epoch 16 Batch 2400 Loss 0.8925
Saving checkpoint for epoch 16 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-8
Starting evaluate ...
Epoch 16 Loss 0.8619; val Loss 1.5135
Time taken for 1 epoch 1257.1123881340027 sec

Epoch 17 Batch 50 Loss 0.8058
Epoch 17 Batch 100 Loss 0.7943
Epoch 17 Batch 150 Loss 0.7998
Epoch 17 Batch 200 Loss 0.8032
Epoch 17 Batch 250 Loss 0.8325
Epoch 17 Batch 300 Loss 0.8258
Epoch 17 Batch 350 Loss 0.7995
Epoch 17 Batch 400 Loss 0.8128
Epoch 17 Batch 450 Loss 0.8364
Epoch 17 Batch 500 Loss 0.8513
Epoch 17 Batch 550 Loss 0.8187
Epoch 17 Batch 600 Loss 0.8364
Epoch 17 Batch 650 Loss 0.8321
Epoch 17 Batch 700 Loss 0.8386
Epoch 17 Batch 750 Loss 0.8478
Epoch 17 Batch 800 Loss 0.8437
Epoch 17 Batch 850 Loss 0.8127
Epoch 17 Batch 900 Loss 0.8379
Epoch 17 Batch 950 Loss 0.8163
Epoch 17 Batch 1000 Loss 0.8400
Epoch 17 Batch 1050 Loss 0.8207
Epoch 17 Batch 1100 Loss 0.8307
Epoch 17 Batch 1150 Loss 0.8579
Epoch 17 Batch 1200 Loss 0.8493
Epoch 17 Batch 1250 Loss 0.8614
Epoch 17 Batch 1300 Loss 0.8415
Epoch 17 Batch 1350 Loss 0.8330
Epoch 17 Batch 1400 Loss 0.8563
Epoch 17 Batch 1450 Loss 0.8502
Epoch 17 Batch 1500 Loss 0.8659
Epoch 17 Batch 1550 Loss 0.8904
Epoch 17 Batch 1600 Loss 0.8624
Epoch 17 Batch 1650 Loss 0.8748
Epoch 17 Batch 1700 Loss 0.8660
Epoch 17 Batch 1750 Loss 0.8510
Epoch 17 Batch 1800 Loss 0.8651
Epoch 17 Batch 1850 Loss 0.8556
Epoch 17 Batch 1900 Loss 0.8622
Epoch 17 Batch 1950 Loss 0.9002
Epoch 17 Batch 2000 Loss 0.8928
Epoch 17 Batch 2050 Loss 0.8459
Epoch 17 Batch 2100 Loss 0.8605
Epoch 17 Batch 2150 Loss 0.8721
Epoch 17 Batch 2200 Loss 0.8675
Epoch 17 Batch 2250 Loss 0.8951
Epoch 17 Batch 2300 Loss 0.9066
Epoch 17 Batch 2350 Loss 0.8737
Epoch 17 Batch 2400 Loss 0.8819
Starting evaluate ...
Epoch 17 Loss 0.8478; val Loss 1.5248
Time taken for 1 epoch 1250.1596565246582 sec

Epoch 18 Batch 50 Loss 0.7909
Epoch 18 Batch 100 Loss 0.7937
Epoch 18 Batch 150 Loss 0.8045
Epoch 18 Batch 200 Loss 0.8203
Epoch 18 Batch 250 Loss 0.7849
Epoch 18 Batch 300 Loss 0.8320
Epoch 18 Batch 350 Loss 0.7940
Epoch 18 Batch 400 Loss 0.7891
Epoch 18 Batch 450 Loss 0.8035
Epoch 18 Batch 500 Loss 0.8015
Epoch 18 Batch 550 Loss 0.8198
Epoch 18 Batch 600 Loss 0.8319
Epoch 18 Batch 650 Loss 0.8329
Epoch 18 Batch 700 Loss 0.8082
Epoch 18 Batch 750 Loss 0.8351
Epoch 18 Batch 800 Loss 0.8189
Epoch 18 Batch 850 Loss 0.8172
Epoch 18 Batch 900 Loss 0.8271
Epoch 18 Batch 950 Loss 0.8322
Epoch 18 Batch 1000 Loss 0.8113
Epoch 18 Batch 1050 Loss 0.8516
Epoch 18 Batch 1100 Loss 0.8406
Epoch 18 Batch 1150 Loss 0.8336
Epoch 18 Batch 1200 Loss 0.8356
Epoch 18 Batch 1250 Loss 0.8476
Epoch 18 Batch 1300 Loss 0.8176
Epoch 18 Batch 1350 Loss 0.8507
Epoch 18 Batch 1400 Loss 0.8463
Epoch 18 Batch 1450 Loss 0.8492
Epoch 18 Batch 1500 Loss 0.8245
Epoch 18 Batch 1550 Loss 0.8122
Epoch 18 Batch 1600 Loss 0.8511
Epoch 18 Batch 1650 Loss 0.8504
Epoch 18 Batch 1700 Loss 0.8426
Epoch 18 Batch 1750 Loss 0.8649
Epoch 18 Batch 1800 Loss 0.8272
Epoch 18 Batch 1850 Loss 0.8765
Epoch 18 Batch 1900 Loss 0.8643
Epoch 18 Batch 1950 Loss 0.8380
Epoch 18 Batch 2000 Loss 0.8576
Epoch 18 Batch 2050 Loss 0.8526
Epoch 18 Batch 2100 Loss 0.8764
Epoch 18 Batch 2150 Loss 0.8571
Epoch 18 Batch 2200 Loss 0.8685
Epoch 18 Batch 2250 Loss 0.8789
Epoch 18 Batch 2300 Loss 0.8586
Epoch 18 Batch 2350 Loss 0.8934
Epoch 18 Batch 2400 Loss 0.8923
Saving checkpoint for epoch 18 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-9
Starting evaluate ...
Epoch 18 Loss 0.8360; val Loss 1.5427
Time taken for 1 epoch 1247.6902573108673 sec

Epoch 19 Batch 50 Loss 0.7969
Epoch 19 Batch 100 Loss 0.7808
Epoch 19 Batch 150 Loss 0.7640
Epoch 19 Batch 200 Loss 0.7916
Epoch 19 Batch 250 Loss 0.7920
Epoch 19 Batch 300 Loss 0.7798
Epoch 19 Batch 350 Loss 0.7864
Epoch 19 Batch 400 Loss 0.7999
Epoch 19 Batch 450 Loss 0.8119
Epoch 19 Batch 500 Loss 0.8038
Epoch 19 Batch 550 Loss 0.8072
Epoch 19 Batch 600 Loss 0.7992
Epoch 19 Batch 650 Loss 0.7938
Epoch 19 Batch 700 Loss 0.7786
Epoch 19 Batch 750 Loss 0.8350
Epoch 19 Batch 800 Loss 0.8191
Epoch 19 Batch 850 Loss 0.8191
Epoch 19 Batch 900 Loss 0.7708
Epoch 19 Batch 950 Loss 0.8266
Epoch 19 Batch 1000 Loss 0.8207
Epoch 19 Batch 1050 Loss 0.8389
Epoch 19 Batch 1100 Loss 0.8303
Epoch 19 Batch 1150 Loss 0.8159
Epoch 19 Batch 1200 Loss 0.8418
Epoch 19 Batch 1250 Loss 0.8146
Epoch 19 Batch 1300 Loss 0.8249
Epoch 19 Batch 1350 Loss 0.8367
Epoch 19 Batch 1400 Loss 0.8236
Epoch 19 Batch 1450 Loss 0.8537
Epoch 19 Batch 1500 Loss 0.8559
Epoch 19 Batch 1550 Loss 0.8191
Epoch 19 Batch 1600 Loss 0.8525
Epoch 19 Batch 1650 Loss 0.8387
Epoch 19 Batch 1700 Loss 0.8459
Epoch 19 Batch 1750 Loss 0.8535
Epoch 19 Batch 1800 Loss 0.8277
Epoch 19 Batch 1850 Loss 0.8526
Epoch 19 Batch 1900 Loss 0.8319
Epoch 19 Batch 1950 Loss 0.8650
Epoch 19 Batch 2000 Loss 0.8571
Epoch 19 Batch 2050 Loss 0.8988
Epoch 19 Batch 2100 Loss 0.8268
Epoch 19 Batch 2150 Loss 0.8383
Epoch 19 Batch 2200 Loss 0.8296
Epoch 19 Batch 2250 Loss 0.8152
Epoch 19 Batch 2300 Loss 0.8541
Epoch 19 Batch 2350 Loss 0.8351
Epoch 19 Batch 2400 Loss 0.8592
Starting evaluate ...
Epoch 19 Loss 0.8236; val Loss 1.5561
Time taken for 1 epoch 1249.3462255001068 sec

Epoch 20 Batch 50 Loss 0.7509
Epoch 20 Batch 100 Loss 0.7634
Epoch 20 Batch 150 Loss 0.7668
Epoch 20 Batch 200 Loss 0.7541
Epoch 20 Batch 250 Loss 0.7760
Epoch 20 Batch 300 Loss 0.7749
Epoch 20 Batch 350 Loss 0.7657
Epoch 20 Batch 400 Loss 0.7862
Epoch 20 Batch 450 Loss 0.8034
Epoch 20 Batch 500 Loss 0.7799
Epoch 20 Batch 550 Loss 0.8100
Epoch 20 Batch 600 Loss 0.8040
Epoch 20 Batch 650 Loss 0.8118
Epoch 20 Batch 700 Loss 0.7990
Epoch 20 Batch 750 Loss 0.8158
Epoch 20 Batch 800 Loss 0.8035
Epoch 20 Batch 850 Loss 0.8189
Epoch 20 Batch 900 Loss 0.8019
Epoch 20 Batch 950 Loss 0.7872
Epoch 20 Batch 1000 Loss 0.8139
Epoch 20 Batch 1050 Loss 0.8536
Epoch 20 Batch 1100 Loss 0.7794
Epoch 20 Batch 1150 Loss 0.8344
Epoch 20 Batch 1200 Loss 0.8078
Epoch 20 Batch 1250 Loss 0.8200
Epoch 20 Batch 1300 Loss 0.8134
Epoch 20 Batch 1350 Loss 0.8173
Epoch 20 Batch 1400 Loss 0.8140
Epoch 20 Batch 1450 Loss 0.7766
Epoch 20 Batch 1500 Loss 0.7946
Epoch 20 Batch 1550 Loss 0.8256
Epoch 20 Batch 1600 Loss 0.8349
Epoch 20 Batch 1650 Loss 0.8552
Epoch 20 Batch 1700 Loss 0.8427
Epoch 20 Batch 1750 Loss 0.8123
Epoch 20 Batch 1800 Loss 0.8152
Epoch 20 Batch 1850 Loss 0.8281
Epoch 20 Batch 1900 Loss 0.8445
Epoch 20 Batch 1950 Loss 0.8538
Epoch 20 Batch 2000 Loss 0.8393
Epoch 20 Batch 2050 Loss 0.8504
Epoch 20 Batch 2100 Loss 0.8379
Epoch 20 Batch 2150 Loss 0.8420
Epoch 20 Batch 2200 Loss 0.8250
Epoch 20 Batch 2250 Loss 0.8471
Epoch 20 Batch 2300 Loss 0.8350
Epoch 20 Batch 2350 Loss 0.8164
Epoch 20 Batch 2400 Loss 0.8526
Saving checkpoint for epoch 20 at /xhp/summary/data/checkpoints/training_checkpoints_seq2seq/ckpt-10
Starting evaluate ...
Epoch 20 Loss 0.8126; val Loss 1.5708
Time taken for 1 epoch 1241.0199348926544 sec

